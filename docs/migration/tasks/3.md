### **Фаза 1: Создание и изоляция новых сервисов (Фокус: Вынос ресурсоемких компонентов, использование `common`)**

**Цель Фазы 1:**

1.  Создать отдельные репозитории/папки для `scraper-service`, `checker-service`, `indexer-service`, `search-service`.
2.  Перенести соответствующую логику в каждый сервис.
3.  Убедиться, что каждый сервис использует *только* свои зависимости (`requirements_*.txt` из Фазы 0).
4.  Убедиться, что каждый сервис использует *общие* компоненты из `common/` (модели, утилиты).
5.  Подготовить `Dockerfile` для каждого сервиса.

---

#### **Задача 11 (обновленная): Создать `scraper-service` с использованием `common/` (Детализация)**

**Цель:** Вынести логику скрапинга в отдельный сервис, изолировав `playwright` и используя `common`.

**Действия:**

1.  **Создание структуры `scraper-service`:**
    *   **Команда:** `mkdir -p scraper-service/{api,scripts,scraper,utils,processors,config,models}`
    *   *Примечание:* Папки `api`, `models`, `processors`, `utils`, `config` создаются для *копирования* файлов, *не входящих* в `common`, и для *обновления импортов*. Файлы *из* `common` будут импортироваться, а не копироваться.

2.  **Копирование файлов в `scraper-service` (только специфичные):**
    *   **`scripts/scrape_tasks.py`:** Основной скрипт CLI.
        *   **Команда:** `cp scripts/scrape_tasks.py scraper-service/scripts/`
        *   **Обновление импортов:** Заменить `from utils.database_manager import DatabaseManager` на `from common.utils.database_manager import DatabaseManager` (если `DatabaseManager` не в `common`). Если `DatabaseManager` *в* `common`, используйте его. Заменить `from scraper.fipi_scraper import FIPIScraper` на `from scraper.fipi_scraper import FIPIScraper`. Заменить `from processors.page_processor import PageProcessingOrchestrator` на `from common.processors.page_processor import PageProcessingOrchestrator`.
    *   **`scraper/fipi_scraper.py`:** Основная логика скрапинга страницы.
        *   **Команда:** `cp scraper/fipi_scraper.py scraper-service/scraper/`
        *   **Обновление импортов:** Заменить `from processors.page_processor import PageProcessingOrchestrator` на `from common.processors.page_processor import PageProcessingOrchestrator`. Заменить `from utils.browser_manager import BrowserManager` на `from utils.browser_manager import BrowserManager` (так как `browser_manager.py` *не* в `common` и копируется сюда). Заменить `from models.problem_builder import ProblemBuilder` на `from common.models.problem_builder import ProblemBuilder`.
    *   **`utils/browser_manager.py`, `utils/browser_pool_manager.py`:** Управление браузером.
        *   **Команды:**
            ```bash
            cp utils/browser_manager.py scraper-service/utils/
            cp utils/browser_pool_manager.py scraper-service/utils/
            ```
        *   **Обновление импортов:** Заменить `from utils.subject_mapping import get_proj_id_for_subject` на `from common.utils.subject_mapping import get_proj_id_for_subject`.
    *   **`processors/page_processor.py`, `processors/block_processor.py`, `utils/metadata_extractor.py`, `utils/downloader.py`, `utils/task_number_inferer.py`, `services/specification.py`, `config/task_number_rules.json`, `utils/fipi_urls.py`, `utils/task_id_utils.py`, `utils/subject_mapping.py`:** Эти файлы *могут* использовать компоненты из `common`. При копировании в `scraper-service`, обновите их импорты *внутри* `scraper-service` на `from common...`.
        *   **Пример (для `page_processor.py`):** `from processors.block_processor import BlockProcessor` -> `from common.processors.block_processor import BlockProcessor`. `from utils.downloader import AssetDownloader` -> `from common.utils.downloader import AssetDownloader`. `from models.problem_builder import ProblemBuilder` -> `from common.models.problem_builder import ProblemBuilder`.

3.  **Обновление `DatabaseManager` в `scraper-service` (если не в `common`):**
    *   Если `DatabaseManager` *не* был перемещен в `common` (например, версия для скрапинга имеет особенности), скопируйте его в `scraper-service/utils/database_manager.py` или `scraper-service/database/database_manager.py`.
    *   **Обновление импортов:** В `scripts/scrape_tasks.py` и других файлах, использующих `DatabaseManager`, убедитесь, что импорт указывает на *локальную* версию в `scraper-service` или на версию из `common`, если она там есть и подходит.

4.  **Создание `requirements_scraping.txt` для `scraper-service`:**
    *   **Команда:** `cp requirements_scraping.txt scraper-service/`
    *   Убедитесь, что он содержит `playwright`, `beautifulsoup4`, `lxml`, `aiofiles`, `click`, `colorama`, `sqlalchemy`, `asyncpg`, `pydantic`, `python-dotenv`.
    *   Убедитесь, что он *не* содержит `fastapi`, `uvicorn`, `qdrant-client`.

5.  **Создание `Dockerfile.scraping`:**
    *   **Команда:** `touch scraper-service/Dockerfile.scraping`
    *   **Содержимое `Dockerfile.scraping`:**
        ```dockerfile
        # --- STAGE 1: Builder ---
        FROM python:3.11-slim as builder

        # Установка системных зависимостей для компиляции
        RUN apt-get update && apt-get install -y --no-install-recommends \
            build-essential \
            gcc \
            g++ \
            && rm -rf /var/lib/apt/lists/*

        WORKDIR /app

        # Копирование зависимостей и установка
        COPY requirements_scraping.txt .
        RUN pip install --user --no-cache-dir -r requirements_scraping.txt
        # Очистка кэша pip в builder стадии
        RUN pip cache purge

        # --- STAGE 2: Runtime ---
        FROM python:3.11-slim as runtime

        # Установка системных зависимостей для Playwright и runtime
        RUN apt-get update && apt-get install -y --no-install-recommends \
            ca-certificates \
            fonts-liberation \
            libappindicator3-1 \
            libasound2 \
            libatk-bridge2.0-0 \
            libatk1.0-0 \
            libc6 \
            libcairo2 \
            libcups2 \
            libdbus-1-3 \
            libexpat1 \
            libfontconfig1 \
            libgbm1 \
            libgcc1 \
            libglib2.0-0 \
            libgtk-3-0 \
            libnspr4 \
            libnss3 \
            libpango-1.0-0 \
            libpangocairo-1.0-0 \
            libstdc++6 \
            libx11-6 \
            libx11-xcb1 \
            libxcb1 \
            libxcomposite1 \
            libxcursor1 \
            libxdamage1 \
            libxext6 \
            libxfixes3 \
            libxi6 \
            libxrandr2 \
            libxrender1 \
            libxss1 \
            libxtst6 \
            wget \
            xdg-utils \
            && rm -rf /var/lib/apt/lists/*

        WORKDIR /app

        # Копирование установленных пакетов из builder
        COPY --from=builder /root/.local /root/.local
        # Убедиться, что исполняемые файлы pip находятся в PATH
        ENV PATH=/root/.local/bin:$PATH

        # Установка Playwright браузеров
        RUN playwright install chromium

        # Копирование common/ библиотеки
        COPY common /app/common
        # Установка common как пакета (или настройка PYTHONPATH)
        RUN pip install --user --no-cache-dir /app/common

        # Копирование кода scraper-service
        COPY scraper-service/ /app/

        # Указание команды запуска (например, скрипта скрапинга)
        # CMD ["python", "scripts/scrape_tasks.py", "run", "math", "informatics"] # Пример
        # Или создайте FastAPI API для запуска скрапинга по запросу
        CMD ["python", "-m", "uvicorn", "api.app:app", "--host", "0.0.0.0", "--port", "10000"] # Если делаете API
        ```
    *   **Примечание:** Убедитесь, что `CMD` соответствует вашему способу запуска скрапинга (CLI скрипт или API эндпоинт).

6.  **Создание `.dockerignore` для `scraper-service`:**
    *   **Команда:** `touch scraper-service/.dockerignore`
    *   **Содержимое:**
        ```
        venv/
        .git/
        tests/
        frontend/
        __pycache__/
        .pytest_cache/
        .vscode/
        .idea/
        *.log
        *.tmp
        *.swp
        data/ # Если не нужно в образе
        docs/
        # Исключить другие сервисы, если они в одном репо
        web-api-service/
        checker-service/
        indexer-service/
        search-service/
        # Исключить корень, если копируете только scraper-service
        ../api/
        ../models/ # Если модели в common
        ../utils/ # Если утилиты в common
        # ... и т.д.
        ```

**Acceptance:**

*   Папка `scraper-service` создана с правильной структурой.
*   Файлы, специфичные для скрапинга, скопированы.
*   Импорты в скопированных файлах обновлены для использования `common/` и локальных файлов.
*   `requirements_scraping.txt` находится в `scraper-service` и содержит правильные зависимости.
*   `Dockerfile.scraping` находится в `scraper-service`, использует `requirements_scraping.txt`, `common/`, устанавливает `playwright`, и имеет корректный `CMD`.
*   `.dockerignore` для `scraper-service` исключает ненужные файлы.

*   **Задача 11 (обновленная): Создать `scraper-service` с использованием `common/`.**
    *   **Дополнительное действие:** После копирования файлов и обновления импортов в `scraper-service`:
        *   Создать `scraper-service/tests/` с подпапками `unit/`, `integration/`.
        *   **Тесты (Unit):**
            *   `scraper-service/tests/unit/test_fipi_scraper.py`: Мокировать `BrowserManager`, `PageProcessingOrchestrator`, `DatabaseManager`. Проверить логику `scrape_page`.
            *   `scraper-service/tests/unit/test_page_processor.py` (если не в `common`): Мокировать `BlockProcessor` и другие зависимости. Проверить логику обработки.
        *   **Тесты (Integration):**
            *   `scraper-service/tests/integration/test_scraping_pipeline.py`: Мокировать `BrowserManager` и `DatabaseManager`. Проверить, что вызов `scripts/scrape_tasks.py` (или API эндпоинта, если реализован) корректно вызывает `FIPIScraper` и `DatabaseManager.save_problems`.
    *   **Acceptance:** `scraper-service` имеет юнит- и интеграционные тесты.

Задача 11 (обновленная): Создать scraper-service с использованием common/.
Дополнительное действие (Документирование):
Создать scraper-service/README.md: Описание сервиса, его назначение, зависимости (requirements_scraping.txt), структура (Dockerfile.scraping, api/, scripts/), как запустить локально, как запустить тесты.
Создать docs/adr/003-scraper-service-architecture.md: ADR для архитектуры scraper-service (использование BrowserPoolManager, интеграция с common, деплой как Private Service).
Создать docs/api/checker-service-openapi.json (если делаете API): Описание API scraper-service (если применимо).
Acceptance (Документирование):
У scraper-service есть собственный README.md.
Существует ADR для архитектуры scraper-service.

Задача 11 (обновленная): Создать scraper-service с использованием common/.
Дополнительное действие (Инженерия требований):
Создать docs/requirements/REQ-005-Scraper-Service.md:
ID: REQ-005
Тип: Функциональное требование + Архитектурное требование.
Требование: "Сервис скрапинга должен извлекать задачи с официального сайта FIPI, обрабатывать их HTML, и сохранять в централизованное хранилище (Supabase). Он должен использовать playwright и изолироваться от веб-API."
Обоснование: Обеспечить актуальность базы задач, не нагружая веб-API.
Критерии приемки: scraper-service запускается, успешно скрапит задачи, использует common компоненты, сохраняет данные в Supabase.
Связанные ADR: docs/adr/003-scraper-service-architecture.md.
Acceptance (Инженерия требований):
Существует формализованное требование REQ-005 для scraper-service.

Задача 11 (обновленная): Создать scraper-service с использованием common/.
Дополнительное действие (Тестирование требований):
Создать tests/requirements/test_req_005_scraper_service.py:
Тест: test_scraper_service_uses_playwright_isolated()
Цель: Проверить, что scraper-service содержит playwright в своих зависимостях (requirements_scraping.txt) и не использует его в web-api-service.
Метод: Прочитать scraper-service/requirements_scraping.txt (проверить наличие playwright), прочитать web-api-service/requirements_web.txt (проверить отсутствие playwright).
Связь с требованием: Покрывает REQ-005.
Acceptance (Тестирование требований):
Существует тест, проверяющий выполнение REQ-005.

*   **Задача 11 (обновленная): Создать `scraper-service` с использованием `common/`.**
    *   **Дополнительное действие (DevEx):**
        *   **Создать `scraper-service/README.md`:**
            *   **Содержание:** Как настроить окружение для `scraper-service`, как запустить его локально (если возможно), как запустить его тесты (`cd scraper-service && pytest tests/unit/`), как запустить скрапинг (например, `python scripts/scrape_tasks.py run math`).
        *   **Обновить общий `Makefile`:**
            *   **Добавить команды:** `test-scraper`, `run-scraper-local` (если применимо).
    *   **Acceptance (DevEx):**
        *   У `scraper-service` есть собственный `README.md` с инструкциями для разработчика.
        *   Есть удобные команды в `Makefile` для работы с `scraper-se
*   **Задача 11 (обновленная): Создать `scraper-service` с использованием `common/`.**
    *   **Дополнительное действие (DevOps/SRE):**
        *   **Создать `scraper-service/Dockerfile.scraping`:** Следует принципам многоступенчатой сборки. *Обязательно* включает `RUN playwright install chromium` *после* установки `playwright` в `builder` стадии, но *до* копирования основного кода в `runtime` стадию. *Не* устанавливает `playwright` в `runtime` стадии веб-API.
        *   **Создать `scraper-service/.dockerignore`:** Исключает ненужные файлы для сборки `scraper-service`.
        *   **Обновить `Makefile` (корень):**
            *   **Добавить команды:** `build-scraper`, `deploy-scraper` (например, `docker build -f scraper-service/Dockerfile.scraping -t ix-scraper . && docker push ...` или вызов Render CLI).
    *   **Acceptance (DevOps/SRE):**
        *   У `scraper-service` есть собственный `Dockerfile.scraping`, оптимизированный для его зависимостей (`playwright`).
        *   Существуют команды в `Makefile` для сборки и деплоя `scraper-service`.rvice`.

*   **Задача 11 (обновленная): Создать `scraper-service` с использованием `common/`.**
    *   **Дополнительное действие (DevSecOps):**
        *   **Обновить `scraper-service/requirements_scraping.txt`:**
            *   Убедиться, что все зависимости проверены на уязвимости (например, через `pip-audit`).
        *   **Обновить `scraper-service/Dockerfile.scraping`:**
            *   Использовать *наименее привилегированный* пользователь (не `root`) в `runtime` стадии, если возможно.
            *   Убедиться, что `playwright install chromium` происходит *внутри* контейнера, а не на хосте.
        *   **Обновить `Makefile`:**
            *   **Добавить команду:** `security-audit-scraper-deps` (например, `pip-audit -r scraper-service/requirements_scraping.txt`).
    *   **Дополнительное действие (GitOps):**
        *   **Обновить `render.yaml`:**
            *   Добавить определение для `scraper-service`.
    *   **Acceptance (DevSecOps):**
        *   Зависимости `scraper-service` проверены на уязвимости.
        *   `Dockerfile.scraping` учитывает безопасность (пользователь, установка `playwright`).
        *   Существует команда для сканирования зависимостей `scraper-service`.
    *   **Acceptance (GitOps):**
        *   `render.yaml` включает определение `scraper-service`.

