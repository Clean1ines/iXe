Фаза 0: Подготовка и анализ (Фокус: Понимание, изоляция, планирование, создание common)
Цель Фазы 0:

Точно определить, какие зависимости используются в каких частях приложения (веб-API, скрапинг, проверка, индексация).
Изолировать тяжелые зависимости (playwright, qdrant-client) от веб-API сервиса.
Выявить общие компоненты (модели, утилиты), которые можно вынести в common/ библиотеку.
Создать структуру common/ и разместить в ней общие компоненты.


Задача 1: Анализ и разделение зависимостей (Детализация)
Цель: Создать отдельные файлы requirements_*.txt для каждого микросервиса, содержащие только необходимые зависимости.

Действия:

Анализ текущего requirements.txt:
Команда: cat requirements.txt
Цель: Получить исходный список зависимостей.
Определение использования зависимостей в api/:
Команда: grep -r "import\|from" api/ --include="*.py" | grep -oE '\b[a-zA-Z_][a-zA-Z0-9_-]*\b' | sort | uniq -c | sort -nr
Альтернатива (более точная): Использовать pipreqs или importchecker:
pipreqs /path/to/your/project/api --force --print
importchecker /path/to/your/project/api (если установлен)
Цель: Получить фактический список пакетов, импортированных в коде api/.
Анализ: Сопоставить список из pipreqs с requirements.txt. Убрать из requirements.txt те пакеты, не используемые в api/. Особое внимание: playwright, qdrant-client, beautifulsoup4, lxml, aiofiles, requests-html, selenium, qdrant_client. Они не должны быть в api/ зависимостях.
Результат: Список зависимостей, необходимых для api/.
Определение использования зависимостей в scraper/ (и scripts/scrape_tasks.py):
Команда: grep -r "import\|from" scraper/ scripts/scrape_tasks.py --include="*.py" | grep -oE '\b[a-zA-Z_][a-zA-Z0-9_-]*\b' | sort | uniq -c | sort -nr
Альтернатива (более точная): pipreqs /path/to/your/project/scripts --force --print && pipreqs /path/to/your/project/scraper --force --print
Цель: Получить список пакетов, используемых в скрапинге.
Анализ: Сопоставить. Включить playwright, beautifulsoup4, lxml, aiofiles, requests, click, colorama.
Результат: Список зависимостей, необходимых для scraper-service.
Определение использования зависимостей в utils/answer_checker.py и utils/browser_manager.py (для checker-service):
Команда: grep -r "import\|from" utils/answer_checker.py utils/browser_manager.py utils/browser_pool_manager.py --include="*.py" | grep -oE '\b[a-zA-Z_][a-zA-Z0-9_-]*\b' | sort | uniq -c | sort -nr
Альтернатива: pipreqs /path/to/your/project/utils --force --print (затем отфильтровать по нужным файлам)
Цель: Получить список пакетов, используемых в проверке ответов.
Анализ: Сопоставить. Включить playwright.
Результат: Список зависимостей, необходимых для checker-service.
Определение использования зависимостей в utils/vector_indexer.py и scripts/index_problems.py (для indexer-service):
Команда: grep -r "import\|from" utils/vector_indexer.py scripts/index_problems.py --include="*.py" | grep -oE '\b[a-zA-Z_][a-zA-Z0-9_-]*\b' | sort | uniq -c | sort -nr
Альтернатива: pipreqs /path/to/your/project/utils --force --print && pipreqs /path/to/your/project/scripts --force --print (затем отфильтровать)
Цель: Получить список пакетов, используемых в индексации.
Анализ: Сопоставить. Включить qdrant-client, numpy, pandas, scikit-learn (если используются).
Результат: Список зависимостей, необходимых для indexer-service.
Определение зависимостей для разработки/тестирования:
Анализ: Проверить, где используются pytest, pytest-asyncio, pytest-cov, black, isort, mypy, pipreqs, pipdeptree, httpie, generate_tree.py.
Результат: Список зависимостей, необходимых для dev-requirements.txt.
Создание requirements_*.txt:
requirements_web.txt: Зависимости, только из анализа api/ (FastAPI, SQLAlchemy, Pydantic, Uvicorn, python-multipart, python-dotenv, jinja2, cryptography, httpx, passlib, python-jose, без playwright, qdrant-client и т.д.).
requirements_scraping.txt: Зависимости, только из анализа scraper/ и scripts/scrape_tasks.py (playwright, beautifulsoup4, lxml, aiofiles, requests, click, colorama, sqlalchemy, pydantic, python-dotenv, без fastapi, uvicorn).
requirements_checking.txt: Зависимости, только из анализа answer_checker.py, browser_manager.py, browser_pool_manager.py (playwright, python-dotenv, без fastapi, uvicorn, sqlalchemy).
requirements_indexing.txt: Зависимости, только из анализа vector_indexer.py, index_problems.py (qdrant-client, sqlalchemy, asyncpg, pydantic, python-dotenv, numpy, pandas, scikit-learn, без fastapi, uvicorn, playwright).
dev-requirements.txt: Зависимости для разработки и тестирования (pytest, black, isort, mypy, pipreqs, pipdeptree, httpie, generate_tree.py, без playwright, qdrant-client и т.д.).
Acceptance:

Созданы файлы requirements_web.txt, requirements_scraping.txt, requirements_checking.txt, requirements_indexing.txt, dev-requirements.txt.
Каждый файл содержит только зависимости, фактически используемые в соответствующем компоненте.
playwright и qdrant-client отсутствуют в requirements_web.txt.
