Хорошо, продолжим в том же духе, детализируя следующие шаги и учитывая уже накопленный контекст (структура проекта, анализ зависимостей, создание `common/`, цели оптимизации).

---

### **Фаза 1: Создание и изоляция новых сервисов (Фокус: Вынос ресурсоемких компонентов, использование `common`)**

**Цель Фазы 1:**

1.  Создать отдельные репозитории/папки для `scraper-service`, `checker-service`, `indexer-service`, `search-service`.
2.  Перенести соответствующую логику в каждый сервис.
3.  Убедиться, что каждый сервис использует *только* свои зависимости (`requirements_*.txt` из Фазы 0).
4.  Убедиться, что каждый сервис использует *общие* компоненты из `common/` (модели, утилиты).
5.  Подготовить `Dockerfile` для каждого сервиса.

---

#### **Задача 11 (обновленная): Создать `scraper-service` с использованием `common/` (Детализация)**

**Цель:** Вынести логику скрапинга в отдельный сервис, изолировав `playwright` и используя `common`.

**Действия:**

1.  **Создание структуры `scraper-service`:**
    *   **Команда:** `mkdir -p scraper-service/{api,scripts,scraper,utils,processors,config,models}`
    *   *Примечание:* Папки `api`, `models`, `processors`, `utils`, `config` создаются для *копирования* файлов, *не входящих* в `common`, и для *обновления импортов*. Файлы *из* `common` будут импортироваться, а не копироваться.

2.  **Копирование файлов в `scraper-service` (только специфичные):**
    *   **`scripts/scrape_tasks.py`:** Основной скрипт CLI.
        *   **Команда:** `cp scripts/scrape_tasks.py scraper-service/scripts/`
        *   **Обновление импортов:** Заменить `from utils.database_manager import DatabaseManager` на `from common.utils.database_manager import DatabaseManager` (если `DatabaseManager` не в `common`). Если `DatabaseManager` *в* `common`, используйте его. Заменить `from scraper.fipi_scraper import FIPIScraper` на `from scraper.fipi_scraper import FIPIScraper`. Заменить `from processors.page_processor import PageProcessingOrchestrator` на `from common.processors.page_processor import PageProcessingOrchestrator`.
    *   **`scraper/fipi_scraper.py`:** Основная логика скрапинга страницы.
        *   **Команда:** `cp scraper/fipi_scraper.py scraper-service/scraper/`
        *   **Обновление импортов:** Заменить `from processors.page_processor import PageProcessingOrchestrator` на `from common.processors.page_processor import PageProcessingOrchestrator`. Заменить `from utils.browser_manager import BrowserManager` на `from utils.browser_manager import BrowserManager` (так как `browser_manager.py` *не* в `common` и копируется сюда). Заменить `from models.problem_builder import ProblemBuilder` на `from common.models.problem_builder import ProblemBuilder`.
    *   **`utils/browser_manager.py`, `utils/browser_pool_manager.py`:** Управление браузером.
        *   **Команды:**
            ```bash
            cp utils/browser_manager.py scraper-service/utils/
            cp utils/browser_pool_manager.py scraper-service/utils/
            ```
        *   **Обновление импортов:** Заменить `from utils.subject_mapping import get_proj_id_for_subject` на `from common.utils.subject_mapping import get_proj_id_for_subject`.
    *   **`processors/page_processor.py`, `processors/block_processor.py`, `utils/metadata_extractor.py`, `utils/downloader.py`, `utils/task_number_inferer.py`, `services/specification.py`, `config/task_number_rules.json`, `utils/fipi_urls.py`, `utils/task_id_utils.py`, `utils/subject_mapping.py`:** Эти файлы *могут* использовать компоненты из `common`. При копировании в `scraper-service`, обновите их импорты *внутри* `scraper-service` на `from common...`.
        *   **Пример (для `page_processor.py`):** `from processors.block_processor import BlockProcessor` -> `from common.processors.block_processor import BlockProcessor`. `from utils.downloader import AssetDownloader` -> `from common.utils.downloader import AssetDownloader`. `from models.problem_builder import ProblemBuilder` -> `from common.models.problem_builder import ProblemBuilder`.

3.  **Обновление `DatabaseManager` в `scraper-service` (если не в `common`):**
    *   Если `DatabaseManager` *не* был перемещен в `common` (например, версия для скрапинга имеет особенности), скопируйте его в `scraper-service/utils/database_manager.py` или `scraper-service/database/database_manager.py`.
    *   **Обновление импортов:** В `scripts/scrape_tasks.py` и других файлах, использующих `DatabaseManager`, убедитесь, что импорт указывает на *локальную* версию в `scraper-service` или на версию из `common`, если она там есть и подходит.

4.  **Создание `requirements_scraping.txt` для `scraper-service`:**
    *   **Команда:** `cp requirements_scraping.txt scraper-service/`
    *   Убедитесь, что он содержит `playwright`, `beautifulsoup4`, `lxml`, `aiofiles`, `click`, `colorama`, `sqlalchemy`, `asyncpg`, `pydantic`, `python-dotenv`.
    *   Убедитесь, что он *не* содержит `fastapi`, `uvicorn`, `qdrant-client`.

5.  **Создание `Dockerfile.scraping`:**
    *   **Команда:** `touch scraper-service/Dockerfile.scraping`
    *   **Содержимое `Dockerfile.scraping`:**
        ```dockerfile
        # --- STAGE 1: Builder ---
        FROM python:3.11-slim as builder

        # Установка системных зависимостей для компиляции
        RUN apt-get update && apt-get install -y --no-install-recommends \
            build-essential \
            gcc \
            g++ \
            && rm -rf /var/lib/apt/lists/*

        WORKDIR /app

        # Копирование зависимостей и установка
        COPY requirements_scraping.txt .
        RUN pip install --user --no-cache-dir -r requirements_scraping.txt
        # Очистка кэша pip в builder стадии
        RUN pip cache purge

        # --- STAGE 2: Runtime ---
        FROM python:3.11-slim as runtime

        # Установка системных зависимостей для Playwright и runtime
        RUN apt-get update && apt-get install -y --no-install-recommends \
            ca-certificates \
            fonts-liberation \
            libappindicator3-1 \
            libasound2 \
            libatk-bridge2.0-0 \
            libatk1.0-0 \
            libc6 \
            libcairo2 \
            libcups2 \
            libdbus-1-3 \
            libexpat1 \
            libfontconfig1 \
            libgbm1 \
            libgcc1 \
            libglib2.0-0 \
            libgtk-3-0 \
            libnspr4 \
            libnss3 \
            libpango-1.0-0 \
            libpangocairo-1.0-0 \
            libstdc++6 \
            libx11-6 \
            libx11-xcb1 \
            libxcb1 \
            libxcomposite1 \
            libxcursor1 \
            libxdamage1 \
            libxext6 \
            libxfixes3 \
            libxi6 \
            libxrandr2 \
            libxrender1 \
            libxss1 \
            libxtst6 \
            wget \
            xdg-utils \
            && rm -rf /var/lib/apt/lists/*

        WORKDIR /app

        # Копирование установленных пакетов из builder
        COPY --from=builder /root/.local /root/.local
        # Убедиться, что исполняемые файлы pip находятся в PATH
        ENV PATH=/root/.local/bin:$PATH

        # Установка Playwright браузеров
        RUN playwright install chromium

        # Копирование common/ библиотеки
        COPY common /app/common
        # Установка common как пакета (или настройка PYTHONPATH)
        RUN pip install --user --no-cache-dir /app/common

        # Копирование кода scraper-service
        COPY scraper-service/ /app/

        # Указание команды запуска (например, скрипта скрапинга)
        # CMD ["python", "scripts/scrape_tasks.py", "run", "math", "informatics"] # Пример
        # Или создайте FastAPI API для запуска скрапинга по запросу
        CMD ["python", "-m", "uvicorn", "api.app:app", "--host", "0.0.0.0", "--port", "10000"] # Если делаете API
        ```
    *   **Примечание:** Убедитесь, что `CMD` соответствует вашему способу запуска скрапинга (CLI скрипт или API эндпоинт).

6.  **Создание `.dockerignore` для `scraper-service`:**
    *   **Команда:** `touch scraper-service/.dockerignore`
    *   **Содержимое:**
        ```
        venv/
        .git/
        tests/
        frontend/
        __pycache__/
        .pytest_cache/
        .vscode/
        .idea/
        *.log
        *.tmp
        *.swp
        data/ # Если не нужно в образе
        docs/
        # Исключить другие сервисы, если они в одном репо
        web-api-service/
        checker-service/
        indexer-service/
        search-service/
        # Исключить корень, если копируете только scraper-service
        ../api/
        ../models/ # Если модели в common
        ../utils/ # Если утилиты в common
        # ... и т.д.
        ```

**Acceptance:**

*   Папка `scraper-service` создана с правильной структурой.
*   Файлы, специфичные для скрапинга, скопированы.
*   Импорты в скопированных файлах обновлены для использования `common/` и локальных файлов.
*   `requirements_scraping.txt` находится в `scraper-service` и содержит правильные зависимости.
*   `Dockerfile.scraping` находится в `scraper-service`, использует `requirements_scraping.txt`, `common/`, устанавливает `playwright`, и имеет корректный `CMD`.
*   `.dockerignore` для `scraper-service` исключает ненужные файлы.

