"""
Main entry point for the FIPI Parser application.

This script orchestrates the scraping process of educational tasks (problems)
from the FIPI website. It provides an interactive command-line interface
to select subjects and initiate scraping. Data is saved in a structured
format (SQLite database and HTML files) to a designated directory.

The scraping is iterative: it starts from the 'init' page and then
proceeds through numbered pages (1, 2, ...) until a specified number
of consecutive empty pages is encountered.

Attributes:
    SUBJECT_ALIAS_MAP (dict): Maps official Russian subject names
                              to shorter Latin aliases for directory names.
    EXAM_YEAR (str): The target exam year for scraped data (currently hardcoded).
"""

import asyncio
import config
from scraper.fipi_scraper import FIPIScraper
from utils.database_manager import DatabaseManager
from utils.logging_config import setup_logging
import logging
from pathlib import Path
import shutil

# === ĞœĞĞŸĞŸĞ˜ĞĞ“: ĞÑ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ â†’ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğ¹ Ğ°Ğ»Ğ¸Ğ°Ñ ===
SUBJECT_ALIAS_MAP = {
    "ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ°. ĞŸÑ€Ğ¾Ñ„Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ": "promath",
    "ĞœĞ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ°. Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ": "basemath",
    "Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸ĞºĞ° Ğ¸ Ğ˜ĞšĞ¢": "inf",
    "Ğ ÑƒÑÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº": "rus",
    "Ğ¤Ğ¸Ğ·Ğ¸ĞºĞ°": "phys",
    "Ğ¥Ğ¸Ğ¼Ğ¸Ñ": "chem",
    "Ğ‘Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ": "bio",
    "Ğ˜ÑÑ‚Ğ¾Ñ€Ğ¸Ñ": "hist",
    "ĞĞ±Ñ‰ĞµÑÑ‚Ğ²Ğ¾Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ": "soc",
    "Ğ›Ğ¸Ñ‚ĞµÑ€Ğ°Ñ‚ÑƒÑ€Ğ°": "lit",
    "Ğ“ĞµĞ¾Ğ³Ñ€Ğ°Ñ„Ğ¸Ñ": "geo",
    "ĞĞ½Ğ³Ğ»Ğ¸Ğ¹ÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº": "eng",
    "ĞĞµĞ¼ĞµÑ†ĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº": "de",
    "Ğ¤Ñ€Ğ°Ğ½Ñ†ÑƒĞ·ÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº": "fr",
    "Ğ˜ÑĞ¿Ğ°Ğ½ÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº": "es",
    "ĞšĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¸Ğ¹ ÑĞ·Ñ‹Ğº": "zh",
}

# === Ğ“ĞĞ” Ğ­ĞšĞ—ĞĞœĞ•ĞĞ (Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ²Ñ‹Ğ½ĞµÑÑ‚Ğ¸ Ğ² config Ğ¿Ğ¾Ğ·Ğ¶Ğµ) ===
EXAM_YEAR = "2026"

def get_subject_output_dir(subject_name: str) -> Path:
    """
    Returns the output directory path for a given subject.

    Constructs the path as data/{alias}/{year}/ based on the subject name.
    If the subject name is not found in the alias map, it creates an alias
    by sanitizing the name.

    Args:
        subject_name (str): The official Russian name of the subject.

    Returns:
        Path: The pathlib.Path object representing the output directory.
    """
    alias = SUBJECT_ALIAS_MAP.get(subject_name)
    if not alias:
        # Fallback: sanitize and use latinized version
        alias = "".join(c if c.isalnum() else "_" for c in subject_name.lower())
        alias = alias[:20]  # limit length
    return Path("data") / alias / EXAM_YEAR

async def main():
    """
    The main asynchronous function providing the CLI loop for scraping.

    It initializes logging, displays a menu, fetches available subjects
    from FIPI, allows the user to select a subject, handles existing data
    (prompting for restart), creates the output directory, initializes
    the database, and then iteratively scrapes pages for the selected subject.
    """
    setup_logging(level="INFO")
    logger = logging.getLogger(__name__)
    logger.info("FIPI Parser Started (Scraping Mode)")

    print("ğŸš€ Welcome to the FIPI Parser!")
    print("ğŸ“‹ 1. Scrape a new subject or update existing data")
    print("ğŸšª 2. Exit")
    print("-" * 40)

    while True:
        choice = input("ğŸ‘‰ Enter your choice (1/2): ").strip()

        if choice == '1':
            print("\nğŸ” Fetching available subjects from FIPI...")
            try:
                scraper = FIPIScraper(
                    base_url=config.FIPI_QUESTIONS_URL,
                    subjects_url=config.FIPI_SUBJECTS_URL
                )
                projects = await asyncio.to_thread(scraper.get_projects)
                if not projects:
                    print("âŒ No subjects found.")
                    continue
            except Exception as e:
                print(f"âŒ Error fetching subjects: {e}")
                logger.error(f"Error fetching subjects: {e}", exc_info=True)
                continue

            project_list = list(projects.items())
            print("\nğŸ“‹ Available subjects:")
            for idx, (proj_id, name) in enumerate(project_list, start=1):
                print(f"{idx}. {name}")
            print(f"{len(project_list) + 1}. Back to Main Menu")

            while True:
                selection_input = input(f"\nğŸ”¢ Enter the number of the subject to scrape (or 'b' to go back): ").strip()
                if selection_input.lower() == 'b':
                    break

                try:
                    selection = int(selection_input)
                    if 1 <= selection <= len(project_list):
                        proj_id, subject_name = project_list[selection - 1]
                        subject_dir = get_subject_output_dir(subject_name)
                        db_path = subject_dir / "fipi_data.db"

                        if db_path.exists():
                            print(f"\nâš ï¸  Data for '{subject_name}' already exists at {subject_dir}.")
                            print("1. Restart scraping (delete existing data)")
                            print("2. Cancel")
                            action = input("Enter choice (1/2): ").strip()
                            if action == '1':
                                shutil.rmtree(subject_dir, ignore_errors=True)
                                print(f"âœ… Deleted existing data in {subject_dir}")
                            else:
                                print("Scraping cancelled.")
                                continue

                        subject_dir.mkdir(parents=True, exist_ok=True)
                        db_manager = DatabaseManager(str(db_path))
                        db_manager.initialize_db()
                        print(f"ğŸ“ Output directory: {subject_dir}")

                        # === Ğ˜Ğ¢Ğ•Ğ ĞĞ¢Ğ˜Ğ’ĞĞ«Ğ™ Ğ¡ĞšĞ ĞĞŸĞ˜ĞĞ“ ===
                        print("ğŸ“„ Scraping pages iteratively until empty...")
                        total_saved = 0

                        # Ğ¡ĞºÑ€Ğ°Ğ¿Ğ¸Ğ¼ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ "init"
                        try:
                            problems, _ = await asyncio.to_thread(
                                scraper.scrape_page,
                                proj_id=proj_id,
                                page_num="init",
                                run_folder=subject_dir
                            )
                            if problems:
                                for problem in problems:
                                    if not getattr(problem, 'subject', None):
                                        alias = SUBJECT_ALIAS_MAP.get(subject_name, "unknown")
                                        # ĞœĞ°Ğ¿Ğ¿Ğ¸Ğ½Ğ³ Ğ°Ğ»Ğ¸Ğ°ÑĞ° â†’ subject key
                                        subject_key_map = {
                                            "promath": "math",
                                            "basemath": "math",
                                            "inf": "informatics",
                                            "rus": "russian",
                                            "phys": "physics",
                                            "chem": "chemistry",
                                            "bio": "biology",
                                            "hist": "history",
                                            "soc": "social",
                                            "lit": "literature",
                                            "geo": "geography",
                                            "eng": "english",
                                            "de": "german",
                                            "fr": "french",
                                            "es": "spanish",
                                            "zh": "chinese",
                                        }
                                        problem.subject = subject_key_map.get(alias, "unknown")
                                db_manager.save_problems(problems)
                                total_saved += len(problems)
                                print(f" âœ… Saved {len(problems)} problems from page init")
                            else:
                                print(" âš ï¸  Page init is empty")
                        except Exception as e:
                            print(f" âŒ Error on page init: {e}")
                            logger.error(f"Error scraping page init: {e}", exc_info=True)

                        # Ğ¡ĞºÑ€Ğ°Ğ¿Ğ¸Ğ¼ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ 1, 2, 3, ...
                        page_num = 1
                        empty_count = 0
                        max_empty = 2

                        while empty_count < max_empty:
                            print(f"ğŸ“„ Trying page {page_num} ...")
                            try:
                                problems, _ = await asyncio.to_thread(
                                    scraper.scrape_page,
                                    proj_id=proj_id,
                                    page_num=str(page_num),
                                    run_folder=subject_dir
                                )
                                if len(problems) == 0:
                                    empty_count += 1
                                    print(f"   âš ï¸  Page {page_num} is empty ({empty_count}/{max_empty})")
                                else:
                                    empty_count = 0
                                    for problem in problems:
                                        if not getattr(problem, 'subject', None):
                                            alias = SUBJECT_ALIAS_MAP.get(subject_name, "unknown")
                                            subject_key_map = {
                                                "promath": "math",
                                                "basemath": "math",
                                                "inf": "informatics",
                                                "rus": "russian",
                                                "phys": "physics",
                                                "chem": "chemistry",
                                                "bio": "biology",
                                                "hist": "history",
                                                "soc": "social",
                                                "lit": "literature",
                                                "geo": "geography",
                                                "eng": "english",
                                                "de": "german",
                                                "fr": "french",
                                                "es": "spanish",
                                                "zh": "chinese",
                                            }
                                            problem.subject = subject_key_map.get(alias, "unknown")
                                    db_manager.save_problems(problems)
                                    total_saved += len(problems)
                                    print(f"   âœ… Saved {len(problems)} problems from page {page_num}")
                            except Exception as e:
                                print(f"   âŒ Error on page {page_num}: {e}")
                                logger.error(f"Error scraping page {page_num}: {e}", exc_info=True)
                                empty_count += 1
                            page_num += 1

                        print(f"\nğŸ‰ Scraping completed! Total problems saved: {total_saved}")
                        logger.info(f"Scraping finished for '{subject_name}', {total_saved} problems saved.")

                    elif selection == len(project_list) + 1:
                        break
                    else:
                        print("âŒ Invalid number.")
                except ValueError:
                    print("âŒ Please enter a valid number.")

        elif choice == '2':
            print("ğŸ‘‹ Goodbye!")
            break
        else:
            print("âŒ Invalid choice. Please enter 1 or 2.")


if __name__ == "__main__":
    asyncio.run(main())
